{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance comparison studies\n",
    "\n",
    "This notebook contains the code to reproduce the results of the performance comparison studies for different BDT and DNN-based models presented in the paper (Figure 7) as well as a study on the impact of a random rotation on a BDT and a DNN model (Figure 8). \n",
    "\n",
    "# Study 1: Performance comparison of different BDT and DNN-based models\n",
    "\n",
    "There are two options to run the code for this study:\n",
    "\n",
    "1. Train all the models yourself. While this is certainly possible with this code, it will require quite some time to train all the ensembles of all the different model types. Consider extracting the code in the respective cells to separate python scripts and run them on a computing cluster.\n",
    "\n",
    "2. Use the pre-trained models. The pre-trained models are available in the `treebased_ad_files/models.zip` file. The code in this notebook will extract the model files and use them for the performance comparison study.\n",
    "\n",
    "**Important note:** For extracting the pre-trained models, it is important to use the exact same version of the python packages that were used to train the models. Therefore, create a `conda` environment using the `environment.yml` file and run the notebook from within this environment if you want to load the models.\n",
    "\n",
    "### Option 1: Train all models yourself\n",
    "\n",
    "First, we import the necessary functions that we'll need for this notebook and then load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import (load_lhco_rd, add_gaussian_features, train_model_multirun,\n",
    "                   load_models_allruns, eval_ensemble, multi_roc_sigeffs,\n",
    "                   random_rotation)\n",
    "from plot_utils import plot_sic_curves, plot_sic_curve_comparison\n",
    "from tmva_utils import train_tmva_multi, eval_tmva_multi\n",
    "from os.path import exists, join\n",
    "from os import mkdir\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.cm as col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** If the below cell gives you loading errors (containing an error message saying that the data cannot be loaded when `allow_pickle=False`), go to the `treebased_ad_files` folder in your terminal and type the following commands:\n",
    "```bash\n",
    "git lfs install\n",
    "git lfs pull\n",
    "```\n",
    "Then restart the kernel of this notebook and try to run again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "data = load_lhco_rd(\"./treebased_ad_files/lhco_rd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.1: Train Histogram Gradient Boosting (HGB) models\n",
    "\n",
    "\n",
    "First, we set the parameters for all the models in terms of how many training runs we want to do and how many models should be contained in a single ensemble. The settings in the cell below reflect those used in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run training with five Gaussian noise features added\n",
    "# How often to re-run the entire ensemble training procedure\n",
    "num_runs = 10\n",
    "\n",
    "# How many models constitute a single ensemble\n",
    "ensembles_per_model = 10\n",
    "\n",
    "max_iters = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train the HGB models, firstly on the original dataset, containing four physics features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train HGB models\n",
    "full_losses_hgb_0G, models_hgb_0G = train_model_multirun(\n",
    "    data,\n",
    "    num_runs=num_runs, ensembles_per_model=ensembles_per_model,\n",
    "    max_iters=max_iters, model_type=\"HGB\", compute_val_weights=True,\n",
    "    save_model_dir=\"./models/models_hgb_0G\",\n",
    "    cv_mode=\"random\", early_stopping=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we train the HGB models on the dataset containing the four physics features and the ten additional features, which are pure Gaussian noise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add noise features\n",
    "data_10G = add_gaussian_features(data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train HGB models\n",
    "full_losses_hgb_10G, models_hgb_10G = train_model_multirun(\n",
    "    data_10G,\n",
    "    num_runs=num_runs, ensembles_per_model=ensembles_per_model,\n",
    "    max_iters=max_iters, model_type=\"HGB\", compute_val_weights=True,\n",
    "    save_model_dir=\"./models/models_hgb_10G\",\n",
    "    cv_mode=\"random\", early_stopping=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.2: Train Adaboost models\n",
    "\n",
    "Again, we first train on the original dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Adaboost models\n",
    "full_losses_ada_0G, models_ada_0G = train_model_multirun(\n",
    "    data,\n",
    "    num_runs=num_runs, ensembles_per_model=ensembles_per_model,\n",
    "    max_iters=max_iters, model_type=\"Ada\", compute_val_weights=True,\n",
    "    save_model_dir=\"./models/models_ada_0G\",\n",
    "    cv_mode=\"random\", early_stopping=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we train on the dataset using the ten Gaussian features added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Adaboost models\n",
    "full_losses_ada_10G, models_ada_10G = train_model_multirun(\n",
    "    data_10G,\n",
    "    num_runs=num_runs, ensembles_per_model=ensembles_per_model,\n",
    "    max_iters=max_iters, model_type=\"Ada\", compute_val_weights=True,\n",
    "    save_model_dir=\"./models/models_ada_10G\",\n",
    "    cv_mode=\"random\", early_stopping=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.3: Train random forest models\n",
    "\n",
    "First on \"vanilla\" dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train random forest models\n",
    "full_losses_rf_0G, models_rf_0G = train_model_multirun(\n",
    "    data,\n",
    "    num_runs=num_runs, ensembles_per_model=ensembles_per_model,\n",
    "    max_iters=max_iters, model_type=\"RF\", compute_val_weights=True,\n",
    "    save_model_dir=\"./models/models_rf_0G\",\n",
    "    cv_mode=\"random\", early_stopping=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then on dataset with ten Gaussian features added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train random forest models\n",
    "full_losses_rf_10G, models_rf_10G = train_model_multirun(\n",
    "    data_10G,\n",
    "    num_runs=num_runs, ensembles_per_model=ensembles_per_model,\n",
    "    max_iters=max_iters, model_type=\"RF\", compute_val_weights=True,\n",
    "    save_model_dir=\"./models/models_rf_10G\",\n",
    "    cv_mode=\"random\", early_stopping=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.4: Train ROOT TMVA models\n",
    "\n",
    "Again, first on the dataset with the four physics features. Note that the interface for the ROOT models is slightly different than for the other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = train_tmva_multi(\n",
    "    data,\n",
    "    num_runs=num_runs, ensembles_per_model=ensembles_per_model,\n",
    "    cv_mode=\"random\",\n",
    "    model_identifier=\"BDT\", root_file_dir_base=\"./models/TMVA_0G\")\n",
    "\n",
    "predictions_tmva_0G = eval_tmva_multi(\n",
    "    data, \n",
    "    num_runs=num_runs, ensembles_per_model=ensembles_per_model, model_identifier=\"BDT\",\n",
    "    root_file_dir_base=\"./models/TMVA_0G\", save_ensemble_preds=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And again on the dataset containing the ten Gaussian features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = train_tmva_multi(\n",
    "    data_10G,\n",
    "    num_runs=num_runs, ensembles_per_model=ensembles_per_model,\n",
    "    cv_mode=\"random\",\n",
    "    model_identifier=\"BDT\", root_file_dir_base=\"./models/TMVA_10G\")\n",
    "\n",
    "predictions_tmva_10G = eval_tmva_multi(\n",
    "    data_10G, \n",
    "    num_runs=num_runs, ensembles_per_model=ensembles_per_model, model_identifier=\"BDT\",\n",
    "    root_file_dir_base=\"./models/TMVA_10G\", save_ensemble_preds=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.5: Train DNN models\n",
    "\n",
    "We start with the \"vanilla\" dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train DNN models\n",
    "full_losses_dnn_0G, models_dnn_0G = train_model_multirun(\n",
    "    data,\n",
    "    num_runs=num_runs, ensembles_per_model=ensembles_per_model,\n",
    "    max_iters=max_iters, model_type=\"DNN\", compute_val_weights=True,\n",
    "    save_model_dir=\"./models/models_dnn_0G\",\n",
    "    cv_mode=\"random\", early_stopping=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And again the dataset with 10 Gaussian noise features added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train DNN models\n",
    "full_losses_dnn_10G, models_dnn_10G = train_model_multirun(\n",
    "    data_10G,\n",
    "    num_runs=num_runs, ensembles_per_model=ensembles_per_model,\n",
    "    max_iters=max_iters, model_type=\"DNN\", compute_val_weights=True,\n",
    "    save_model_dir=\"./models/models_dnn_10G\",\n",
    "    cv_mode=\"random\", early_stopping=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional step: Instead of training the models yourself, load the pre-trained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract models if not already done\n",
    "if not exists(\"treebased_ad_files/models\"):\n",
    "    from utils import extract_models\n",
    "    extract_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model files\n",
    "**NOTE:** We load all models except for the RF and the TMVA models, since there the model files are just too large to be stored on the repository. However, we stored the signal and background efficiencies for these models instead, so that we can still compare them to the other models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_ada_0G = load_models_allruns(\"./treebased_ad_files/models/models_ADA_0G\")\n",
    "models_ada_10G = load_models_allruns(\"./treebased_ad_files/models/models_ADA_10G\")\n",
    "models_hgb_0G = load_models_allruns(\"./treebased_ad_files/models/models_HGB_0G\")\n",
    "models_hgb_10G = load_models_allruns(\"./treebased_ad_files/models/models_HGB_10G\")\n",
    "\n",
    "# load signal efficiency and background efficiency (tpr and fpr) for random forest and TMVA models\n",
    "tpr_rf_0G = np.load(\"./treebased_ad_files/models/models_RF_0G/tpr_RF_0G.npy\")\n",
    "fpr_rf_0G = np.load(\"./treebased_ad_files/models/models_RF_0G/fpr_RF_0G.npy\")\n",
    "tpr_rf_10G = np.load(\"./treebased_ad_files/models/models_RF_10G/tpr_RF_10G.npy\")\n",
    "fpr_rf_10G = np.load(\"./treebased_ad_files/models/models_RF_10G/fpr_RF_10G.npy\")\n",
    "\n",
    "tpr_tmva_0G = np.load(\"./treebased_ad_files/models/models_TMVA_0G/tpr_TMVA_0G.npy\")\n",
    "fpr_tmva_0G = np.load(\"./treebased_ad_files/models/models_TMVA_0G/fpr_TMVA_0G.npy\")\n",
    "tpr_tmva_10G = np.load(\"./treebased_ad_files/models/models_TMVA_10G/tpr_TMVA_10G.npy\")\n",
    "fpr_tmva_10G = np.load(\"./treebased_ad_files/models/models_TMVA_10G/fpr_TMVA_10G.npy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For loaded models, run ensemble evaluation and also comput the signal and background efficiencies:\n",
    "\n",
    "**NOTE:** The following cell will take a while to run. Last time it was tested, it took around 10 minutes on a modern-era CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get model predictions\n",
    "predictions_ada_0G = eval_ensemble(models_ada_0G, data, model_type=\"Ada\")\n",
    "predictions_ada_10G = eval_ensemble(models_ada_10G, data_10G, model_type=\"Ada\")\n",
    "predictions_hgb_0G = eval_ensemble(models_hgb_0G, data, model_type=\"HGB\")\n",
    "predictions_hgb_10G = eval_ensemble(models_hgb_10G, data_10G, model_type=\"HGB\")\n",
    "\n",
    "# Compute signal efficiency and background efficiency (tpr and fpr) for all models\n",
    "tpr_ada_0G, fpr_ada_0G = multi_roc_sigeffs(predictions_ada_0G, data[\"y_test\"])\n",
    "tpr_ada_10G, fpr_ada_10G = multi_roc_sigeffs(predictions_ada_10G, data_10G[\"y_test\"])\n",
    "tpr_hgb_0G, fpr_hgb_0G = multi_roc_sigeffs(predictions_hgb_0G, data[\"y_test\"])\n",
    "tpr_hgb_10G, fpr_hgb_10G = multi_roc_sigeffs(predictions_hgb_10G, data_10G[\"y_test\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.6: Create SIC curves for all models\n",
    "\n",
    "We start again on the initial dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set RC params\n",
    "plt.rcParams['pgf.rcfonts'] = False\n",
    "plt.rcParams['font.serif'] = []\n",
    "plt.rcParams['axes.formatter.useoffset'] = False\n",
    "plt.rcParams['lines.linewidth'] = 2\n",
    "plt.rcParams['errorbar.capsize'] = 2\n",
    "plt.rcParams['grid.linewidth'] = 0.5\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['axes.titlesize'] = 16\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "plt.rcParams['legend.title_fontsize'] = 12\n",
    "plt.rcParams['legend.fontsize'] = 12\n",
    "plt.rcParams['legend.frameon'] = False\n",
    "\n",
    "color_list = [\"black\", \"red\", \"orange\", \"dodgerblue\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not exists(\"plots\"):\n",
    "    mkdir(\"plots\")\n",
    "\n",
    "plot_sic_curves([tpr_hgb_0G, tpr_rf_0G, tpr_ada_0G, tpr_tmva_0G],\n",
    "                [fpr_hgb_0G, fpr_rf_0G, fpr_ada_0G, fpr_tmva_0G],\n",
    "                5*[data[\"y_test\"]],\n",
    "                out_filename=join(\"plots\", \"model_comparison_0G.pdf\"),\n",
    "                labels=[\"HGB\", \"RF\", \"Adaboost\", \"TMVA BDT\"],\n",
    "                xlabel=r\"$\\epsilon_{S}$\",\n",
    "                color_list=color_list,\n",
    "                title=\"Baseline\",\n",
    "                ylabel=r\"$\\epsilon_S/\\sqrt{\\epsilon_B}$\",\n",
    "                max_y=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then on the dataset with the ten Gaussian noise features added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sic_curves([tpr_hgb_10G, tpr_rf_10G, tpr_ada_10G, tpr_tmva_10G],\n",
    "                [fpr_hgb_10G, fpr_rf_10G, fpr_ada_10G, fpr_tmva_10G],\n",
    "                5*[data_10G[\"y_test\"]],\n",
    "                out_filename=join(\"plots\", \"rotation_comparison_10G.pdf\"),\n",
    "                labels=[\"HGB\", \"RF\", \"Adaboost\", \"TMVA BDT\"],\n",
    "                xlabel=r\"$\\epsilon_{S}$\",\n",
    "                color_list=color_list,\n",
    "                title=\"Baseline + 10G\",\n",
    "                ylabel=r\"$\\epsilon_S/\\sqrt{\\epsilon_B}$\",\n",
    "                max_y=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Study 2: Impact of random rotation on BDT and DNN models\n",
    "\n",
    "Testing the performance of BDT and DNN models under random rotations of the input data. The cells below can be used to reproduce figure 8 from the paper.\n",
    "\n",
    "Again there are two ways to run the code:\n",
    "- either train the models yourself (which will take some time and optimally should be conducted on a computing cluster)\n",
    "- or load the pre-trained models\n",
    "\n",
    "First, let's load the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data - for this comparison, we need the original data as well as the data with three Gaussian noise features added\n",
    "data = load_lhco_rd(\"./treebased_ad_files/lhco_rd\")\n",
    "data_3G = add_gaussian_features(data, 3)\n",
    "\n",
    "# We also need the same data but with a random rotation applied to the features\n",
    "data_rotated = random_rotation(data)\n",
    "data_3G_rotated = random_rotation(data_3G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 1: Train all models yourself\n",
    "\n",
    "### Step 2.1: Train HGB models\n",
    "\n",
    "Use the following settings for all trainings in this study:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-run training with five Gaussian noise features added\n",
    "# How often to re-run the entire ensemble training procedure\n",
    "num_runs = 10\n",
    "\n",
    "# How many models constitute a single ensemble\n",
    "ensembles_per_model = 10\n",
    "\n",
    "max_iters = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by training HGB models on the initial dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train HGB models\n",
    "full_losses_hgb_0G, models_hgb_0G = train_model_multirun(\n",
    "    data,\n",
    "    num_runs=num_runs, ensembles_per_model=ensembles_per_model,\n",
    "    max_iters=max_iters, model_type=\"HGB\", compute_val_weights=True,\n",
    "    save_model_dir=\"./models/models_hgb_0G\",\n",
    "    cv_mode=\"random\", early_stopping=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then on the dataset with the three Gaussian noise features added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train HGB models\n",
    "full_losses_hgb_3G, models_hgb_3G = train_model_multirun(\n",
    "    data_3G,\n",
    "    num_runs=num_runs, ensembles_per_model=ensembles_per_model,\n",
    "    max_iters=max_iters, model_type=\"HGB\", compute_val_weights=True,\n",
    "    save_model_dir=\"./models/models_hgb_3G\",\n",
    "    cv_mode=\"random\", early_stopping=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train the same models, but this time on the rotated features. We start again with the initial dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train HGB models\n",
    "full_losses_hgb_0G_rotated, models_hgb_0G_rotated = train_model_multirun(\n",
    "    data_rotated,\n",
    "    num_runs=num_runs, ensembles_per_model=ensembles_per_model,\n",
    "    max_iters=max_iters, model_type=\"HGB\", compute_val_weights=True,\n",
    "    save_model_dir=\"./models/models_hgb_0G_rotated\",\n",
    "    cv_mode=\"random\", early_stopping=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then on the dataset with the three Gaussian noise features added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train HGB models\n",
    "full_losses_hgb_3G_rotated, models_hgb_3G_rotated = train_model_multirun(\n",
    "    data_3G_rotated,\n",
    "    num_runs=num_runs, ensembles_per_model=ensembles_per_model,\n",
    "    max_iters=max_iters, model_type=\"HGB\", compute_val_weights=True,\n",
    "    save_model_dir=\"./models/models_hgb_3G_rotated\",\n",
    "    cv_mode=\"random\", early_stopping=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.2: Train DNN models\n",
    "\n",
    "Now we repeat the exact same study for DNN models. Again, we start with the initial dataset without rotations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train DNN models\n",
    "full_losses_dnn_0G, models_dnn_0G = train_model_multirun(\n",
    "    data,\n",
    "    num_runs=num_runs, ensembles_per_model=ensembles_per_model,\n",
    "    max_iters=max_iters, model_type=\"DNN\", compute_val_weights=True,\n",
    "    save_model_dir=\"./models/models_dnn_0G\",\n",
    "    cv_mode=\"random\", early_stopping=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then on the dataset with the three Gaussian noise features added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train DNN models\n",
    "full_losses_dnn_3G, models_dnn_3G = train_model_multirun(\n",
    "    data_3G,\n",
    "    num_runs=num_runs, ensembles_per_model=ensembles_per_model,\n",
    "    max_iters=max_iters, model_type=\"DNN\", compute_val_weights=True,\n",
    "    save_model_dir=\"./models/models_dnn_3G\",\n",
    "    cv_mode=\"random\", early_stopping=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we also retrain the DNN models on the rotated features. Again, we start with the initial dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train DNN models\n",
    "full_losses_dnn_0G_rotated, models_dnn_0G_rotated = train_model_multirun(\n",
    "    data_rotated,\n",
    "    num_runs=num_runs, ensembles_per_model=ensembles_per_model,\n",
    "    max_iters=max_iters, model_type=\"DNN\", compute_val_weights=True,\n",
    "    save_model_dir=\"./models/models_dnn_0G_rotated\",\n",
    "    cv_mode=\"random\", early_stopping=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then on the dataset with the three Gaussian noise features added:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train DNN models\n",
    "full_losses_dnn_3G_rotated, models_dnn_3G_rotated = train_model_multirun(\n",
    "    data_3G_rotated,\n",
    "    num_runs=num_runs, ensembles_per_model=ensembles_per_model,\n",
    "    max_iters=max_iters, model_type=\"DNN\", compute_val_weights=True,\n",
    "    save_model_dir=\"./models/models_dnn_3G_rotated\",\n",
    "    cv_mode=\"random\", early_stopping=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Load pre-trained models\n",
    "\n",
    "Instead of running all the trainings, we can simply load the pre-trained models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract models if not already done\n",
    "if not exists(\"treebased_ad_files/models\"):\n",
    "    from utils import extract_models\n",
    "    extract_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important note! For DNN, the is_dnn parameter has to be set to True\n",
    "models_dnn_0G = load_models_allruns(\"./treebased_ad_files/models/models_DNN_0G\", is_dnn=True)\n",
    "models_dnn_3G = load_models_allruns(\"./treebased_ad_files/models/models_DNN_3G\", is_dnn=True)\n",
    "models_dnn_0G_rotated = load_models_allruns(\"./treebased_ad_files/models/models_DNN_0G_rotated\", is_dnn=True)\n",
    "models_dnn_3G_rotated = load_models_allruns(\"./treebased_ad_files/models/models_DNN_3G_rotated\", is_dnn=True)\n",
    "models_hgb_0G = load_models_allruns(\"./treebased_ad_files/models/models_HGB_0G\")\n",
    "models_hgb_3G = load_models_allruns(\"./treebased_ad_files/models/models_HGB_3G\")\n",
    "models_hgb_0G_rotated = load_models_allruns(\"./treebased_ad_files/models/models_HGB_0G_rotated\")\n",
    "models_hgb_3G_rotated = load_models_allruns(\"./treebased_ad_files/models/models_HGB_3G_rotated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.3: Create SIC curves for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define colors and linestyles for plots\n",
    "colormap_gaussian = col.get_cmap(\"viridis\")\n",
    "N=6\n",
    "col_3G_val = 2.5/N\n",
    "col_3G = colormap_gaussian(col_3G_val)\n",
    "color_list = [\"black\", \"black\", col_3G, col_3G]\n",
    "linestyles = [\"solid\", \"dashed\", \"solid\", \"dashed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set RC params\n",
    "plt.rcParams['pgf.rcfonts'] = False\n",
    "plt.rcParams['font.serif'] = []\n",
    "plt.rcParams['axes.formatter.useoffset'] = False\n",
    "plt.rcParams['lines.linewidth'] = 2\n",
    "plt.rcParams['errorbar.capsize'] = 2\n",
    "plt.rcParams['grid.linewidth'] = 0.5\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['axes.titlesize'] = 16\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "plt.rcParams['legend.title_fontsize'] = 12\n",
    "plt.rcParams['legend.fontsize'] = 12\n",
    "plt.rcParams['legend.frameon'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if plots directory exists\n",
    "if not exists(\"plots\"):\n",
    "    mkdir(\"plots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create SIC curves for the HGB models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sic_curve_comparison([models_hgb_0G, models_hgb_0G_rotated, models_hgb_3G, models_hgb_3G_rotated],\n",
    "                          [data, data_rotated, data_3G, data_3G_rotated],\n",
    "                          model_types=[\"HGB\", \"HGB\", \"HGB\", \"HGB\"],\n",
    "                          out_filename=join(\"plots\", \"rotation_comparison_HGB.pdf\"),\n",
    "                          color_list=color_list,\n",
    "                          linestyles=linestyles,\n",
    "                          labels=[\"Baseline\", \"Baseline rotated\", \"Baseline + 3G\", \"Baseline + 3G rotated\"],\n",
    "                          xlabel=r\"$\\epsilon_{S}$\",\n",
    "                          ylabel=r\"$\\epsilon_S/\\sqrt{\\epsilon_B}$\",\n",
    "                          max_y=20,\n",
    "                          title=\"BDT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for the DNN models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sic_curve_comparison([models_dnn_0G, models_dnn_0G_rotated, models_dnn_3G, models_dnn_3G_rotated],\n",
    "                          [data, data_rotated, data_3G, data_3G_rotated],\n",
    "                          model_types=[\"DNN\", \"DNN\", \"DNN\", \"DNN\"],\n",
    "                          out_filename=join(\"plots\", \"rotation_comparison_DNN.pdf\"),\n",
    "                          color_list=color_list,\n",
    "                          linestyles=linestyles,\n",
    "                          labels=[\"Baseline\", \"Baseline rotated\", \"Baseline + 3G\", \"Baseline + 3G rotated\"],\n",
    "                          xlabel=r\"$\\epsilon_{S}$\",\n",
    "                          ylabel=r\"$\\epsilon_S/\\sqrt{\\epsilon_B}$\",\n",
    "                          max_y=20,\n",
    "                          title=\"NN\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tree_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
